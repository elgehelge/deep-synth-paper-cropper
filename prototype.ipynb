{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "\n",
    "This notebook contains the initial experimentation on corner detection based on neural nets and synthesized data.\n",
    "\n",
    "It is shown that the network is able the generalize from clean computer generated black and white images to somewhat noisy real world photos. The model struggle with photos that does not have high contrast, but this is expected since the training data is black and white images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making Wand work on my machine :(\n",
    "import os\n",
    "os.environ['MAGICK_HOME'] = \"/usr/local/opt/imagemagick@6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elgehelge/.virtualenvs/deep_receipt_cropper/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "import random\n",
    "import glob\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "\n",
    "from gen_train_data import CornerImageGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "IMAGE_WIDTH = 15\n",
    "IMAGE_HEIGHT = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANIAAAD8CAYAAAACP/oEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlsBBMkvgIJklcJDMEvw/SohcTyFYWxQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115e41400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_gen = CornerImageGenerator(image_width=IMAGE_WIDTH, image_height=IMAGE_HEIGHT).generator()    \n",
    "\n",
    "# How it works\n",
    "random_img, random_coord = next(img_gen)\n",
    "plt.imshow(random_img, cmap='gray')\n",
    "plt.scatter(random_coord[0], random_coord[1], s=50, c='red', marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we will just focus on predicting the x-coordinate of the corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " [[ 1.   1.   1.   1.   1.   1.   1.   0.3  0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 1.   1.   1.   1.   1.   1.   1.   1.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 1.   1.   1.   1.   1.   1.   1.   1.   0.4  0.   0.   0.   0.   0.   0. ]\n",
      " [ 1.   1.   1.   1.   1.   1.   1.   1.   1.   0.1  0.   0.   0.   0.   0. ]\n",
      " [ 1.   1.   1.   1.   1.   1.   1.   1.   1.   0.5  0.   0.   0.   0.   0. ]\n",
      " [ 1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   0.1  0.   0.   0.   0. ]\n",
      " [ 1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   0.6  0.   0.   0.   0. ]\n",
      " [ 1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   0.1  0.   0.   0. ]\n",
      " [ 1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   0.7  0.   0.   0. ]\n",
      " [ 1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   0.2  0.   0. ]\n",
      " [ 1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   0.9  0.   0. ]\n",
      " [ 1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   0.3  0. ]\n",
      " [ 1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   0. ]\n",
      " [ 0.7  0.6  0.5  0.4  0.3  0.3  0.2  0.1  0.1  0.1  0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]]\n",
      "y:\n",
      " 13\n",
      "y (one-hot):\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "def flatten_and_normalize(image):\n",
    "    assert isinstance(image, np.ndarray)\n",
    "    assert 0 <= image.min() and image.max() <= 255\n",
    "    flat_img = image.reshape(-1)  # flattened image\n",
    "    flat_norm_img = flat_img / 255\n",
    "    return flat_norm_img\n",
    "\n",
    "def coord_to_horisontal_onehot(max_width, coord):\n",
    "    assert 0 <= coord[0] and coord[0] <= max_width\n",
    "    x_pos = coord[0]  # horisontal position of corner\n",
    "    one_hot = np.zeros(max_width, dtype=int)\n",
    "    one_hot[x_pos] = True\n",
    "    return one_hot\n",
    "\n",
    "# How it works (reshaping and rounding to make it look nice in the notebook)\n",
    "random_img_prep_x = flatten_and_normalize(random_img)\n",
    "random_img_prep_y = coord_to_horisontal_onehot(IMAGE_WIDTH-1, random_coord)\n",
    "assert random_img_prep_x.shape == (IMAGE_HEIGHT * IMAGE_WIDTH, )\n",
    "print('x:\\n', random_img_prep_x.reshape(IMAGE_HEIGHT, IMAGE_WIDTH).round(1))\n",
    "print('y:\\n', random_coord[0])\n",
    "print('y (one-hot):\\n', random_img_prep_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_x_y():\n",
    "    while True:\n",
    "        image, coord = next(img_gen)\n",
    "        x = flatten_and_normalize(image)\n",
    "        y = random_coord[0]\n",
    "        yield x, y\n",
    "        \n",
    "def generator_x_y_onehot():\n",
    "    while True:\n",
    "        image, coord = next(img_gen)\n",
    "        x = flatten_and_normalize(image)\n",
    "        y = coord_to_horisontal_onehot(IMAGE_WIDTH, coord)\n",
    "        yield x, y\n",
    "\n",
    "def generate_batch(generator, size):\n",
    "    images, coords = list(zip(*islice(generator, size)))\n",
    "    return np.array(images), np.array(coords).reshape(size, -1)\n",
    "\n",
    "# How it works\n",
    "next_batch = generate_batch(generator_x_y(), 3)\n",
    "assert (next_batch[0].shape, next_batch[1].shape) == ((3, 300), (3, 1))\n",
    "next_batch = generate_batch(generator_x_y_onehot(), 3)\n",
    "assert (next_batch[0].shape, next_batch[1].shape) == ((3, 300), (3, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the neural network\n",
    "\n",
    "This model is based on as simple Tensorflow MNIST tutorial. No other parameters has been tried out.\n",
    "\n",
    "We will experiment with both classification (with onehot encoded coordinates) and regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this toggle to switch between classification and regression\n",
    "\n",
    "# TASK = 'regression'\n",
    "TASK = 'classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(\n",
    "        x,\n",
    "        W,\n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(\n",
    "        x,\n",
    "        ksize=[1, 2, 2, 1],\n",
    "        strides=[1, 2, 2, 1],\n",
    "        padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK == 'classification':\n",
    "    no_output_classes = IMAGE_WIDTH\n",
    "if TASK == 'regression':\n",
    "    no_output_classes = 1\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None, IMAGE_HEIGHT * IMAGE_WIDTH))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, no_output_classes))\n",
    "\n",
    "x_2dim = tf.reshape(x, [-1, IMAGE_HEIGHT, IMAGE_WIDTH, 1])\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_2dim, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)  # (20, 15) -> (10, 8)\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)  # (10, 8) -> (5, 4)\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 5 * 4 * 64])\n",
    "\n",
    "W_fc1 = weight_variable([5 * 4 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "W_fc2 = weight_variable([1024, no_output_classes])\n",
    "b_fc2 = bias_variable([no_output_classes])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK == 'classification':\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "if TASK == 'regression':\n",
    "    loss = tf.reduce_mean((y_ - y_conv)**2)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "# Only relavant for classification task, but it does no harm to do the computation\n",
    "onehot_mse = tf.reduce_mean((tf.argmax(y_conv, 1) - tf.argmax(y_, 1)) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (50, 1) for Tensor 'Placeholder_1:0', which has shape '(?, 15)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-dc756a47c853>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mITERATIONS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_x_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mTASK\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'classification'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/deep_receipt_cropper/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m   2040\u001b[0m         \u001b[0mnone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2041\u001b[0m     \"\"\"\n\u001b[0;32m-> 2042\u001b[0;31m     \u001b[0m_run_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2044\u001b[0m \u001b[0m_gradient_registry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegistry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gradient\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/deep_receipt_cropper/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[0;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   4488\u001b[0m                        \u001b[0;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4489\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 4490\u001b[0;31m   \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/deep_receipt_cropper/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/deep_receipt_cropper/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1094\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1097\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (50, 1) for Tensor 'Placeholder_1:0', which has shape '(?, 15)'"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 2000\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(ITERATIONS):\n",
    "    batch = generate_batch(generator_x_y(), 50)\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "    if i % 100 == 0:\n",
    "        if TASK == 'classification':\n",
    "            train_mse = onehot_mse.eval(feed_dict={x: batch[0], y_: batch[1]})\n",
    "            print('step %d, training error (onehot) %g' % (i, train_mse))\n",
    "        if TASK == 'regression':\n",
    "            print('step %d, training error %g' % (i, loss.eval(feed_dict={x: batch[0], y_: batch[1]})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and evaluating the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = generate_batch(generator_x_y(), 500)\n",
    "print(\"Test error:\", sess.run(loss, feed_dict={x: test_batch[0], y_: test_batch[1]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1337)\n",
    "\n",
    "for i in range(10):\n",
    "    prep_sample = next(generator_x_y())\n",
    "    pred = y_conv.eval(feed_dict={x: [prep_sample[0]]})\n",
    "\n",
    "    print(\"Input data:\")\n",
    "    plt.imshow(prep_sample[0].reshape(IMAGE_HEIGHT, IMAGE_WIDTH), cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Predicttion:\")\n",
    "    if TASK == 'classification':\n",
    "        plt.figure(figsize=(3,1))\n",
    "        plt.imshow(pred, cmap='Reds')\n",
    "        plt.show()\n",
    "    if TASK == 'regression':\n",
    "        print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_img_file in glob.glob('./test_photos/*'):\n",
    "    orig = skimage.io.imread(test_img_file, as_grey=True)\n",
    "    down_scaled = skimage.transform.resize(orig, output_shape=(IMAGE_HEIGHT, IMAGE_WIDTH), mode='constant')\n",
    "    \n",
    "    prep_img = down_scaled.reshape(-1) \n",
    "    pred = y_conv.eval(feed_dict={x: [prep_img]})\n",
    "    \n",
    "    print(\"Original:\")\n",
    "    plt.imshow(orig, cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Down scaled:\")\n",
    "    plt.imshow(down_scaled, cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Predicted x-coordinates:\")\n",
    "    plt.figure(figsize=(3,1))\n",
    "    plt.imshow(pred, cmap='Reds')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_img_file in glob.glob('./test_photos/*'):\n",
    "    orig = skimage.io.imread(test_img_file, as_grey=True)\n",
    "    down_scaled = skimage.transform.resize(orig, output_shape=(IMAGE_HEIGHT, IMAGE_WIDTH), mode='constant')\n",
    "    high_contrast = skimage.exposure.adjust_sigmoid(down_scaled)\n",
    "    \n",
    "    prep_img = high_contrast.reshape(-1) \n",
    "    pred = y_conv.eval(feed_dict={x: [prep_img]})\n",
    "    \n",
    "    print(\"Original:\")\n",
    "    plt.imshow(orig, cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Down scaled:\")\n",
    "    plt.imshow(high_contrast, cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Predicted x-coordinates:\")\n",
    "    plt.figure(figsize=(3,1))\n",
    "    plt.imshow(pred, cmap='Reds')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
